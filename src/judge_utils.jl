using BenchmarkCI: printresultmd, CIResult
using BenchmarkTools
using PkgBenchmark
using Markdown
using SHA

const RESULTS_BRANCH = "benchmark-results"

const BENCHMARK_RESULT_PREFIX = "fluxml-benchmark-result-"
const BASELINE_RESULT_FILE_NAME = "result-baseline.json"
const TARGET_RESULT_FILE_NAME = "result-target.json"


"""
    gen_result_filename(single_deps_list::String)::String

is used to generate the filename of benchmark results.
The name of results doesn't have relationship with BenchmarkResults,
but highly relevant to dependencies. With the help of this function,
we can use the information of dependencies to specify a file.

* single_deps_list: a part of command argument, `deps-list`.
                    `target` and `baseline` also can passed in.
"""
function gen_result_filename(single_deps_list::String)::String
    # keep deps in sort, in case impact of different sequence.
    sorted_deps = reduce((a, b) -> "$a,$b",
        sort(map(string, split(single_deps_list, ","))))
    return bytes2hex(sha256(sorted_deps))
end


"""
    suitable_to_use_result_cache(single_deps_list::String)

is used to judge if this deps list can use results cache. It cannnot use cache
in the following cases:

1. exists a dependency without version or revision
2. exists a dependency with revision but it's a branch name
"""
function suitable_to_use_result_cache(single_deps_list::String)

end

"""
    get_result_files_from_artifacts(base_path::String)::Tuple{Vector{String}, Vector{String}}

is used to get all the JSON output generated by PkgBenchmark.benchmarkpkg.

To accelerate benchmarking and avoid OOM, this tool splits the whole process
into several small tasks. Each of those task generates its own output.

After benchmarking, result-baseline.json and result-target.json are both in
"benchmark/". The workflow uses "upload-artifact" action to upload these JSON
files, while latter jobs use "download-artifact" action to download them. (
all artifacts are included a directory prefixed with "fluxml-benchmark-result-")

In MergeReport job, it will download all the results. And this function aims
to find all the "result-baseline.json" and "result-target.json".

* base_path: to specify the path artifacts located, "." by default
"""
function get_result_files_from_artifacts(base_path::String)::Tuple{Vector{String}, Vector{String}}
    baseline_results = []
    target_results = []
    @info readdir(base_path)
    all_benchmark_result_dirs = [
        f for f in readdir(base_path) if isdir(f) &&
        startswith(f, BENCHMARK_RESULT_PREFIX)
    ]
    @info all_benchmark_result_dirs
    for dir in all_benchmark_result_dirs
        result_dir_contents = readdir(dir)
        BASELINE_RESULT_FILE_NAME in result_dir_contents ||
            error("$BASELINE_RESULT_FILE_NAME is not in $dir")
        TARGET_RESULT_FILE_NAME in result_dir_contents ||
            error("$TARGET_RESULT_FILE_NAME is not in $dir")
        push!(baseline_results, "$dir/$BASELINE_RESULT_FILE_NAME")
        push!(target_results, "$dir/$TARGET_RESULT_FILE_NAME")
    end
    return (baseline_results, target_results)
end


"""
    merge_benchmarkgroup(a::BenchmarkGroup, b::BenchmarkGroup)::BenchmarkGroup

is used to merge two different BenchmarkGroup.
"""
function merge_benchmarkgroup(a::BenchmarkGroup, b::BenchmarkGroup)::BenchmarkGroup
    out = deepcopy(a)
    for (key, bg) in b
        out[key] = haskey(a, key) ?
            merge_benchmarkgroup(a[key], bg) : bg
    end
    return out
end


"""
    merge_results(a::BenchmarkResults, b::BenchmarkResults)::BenchmarkResults

is used to merge two different BenchmarkResults.
"""
function merge_results(a::BenchmarkResults, b::BenchmarkResults)::BenchmarkResults
    return BenchmarkResults(
        a.name,
        a.commit,
        merge_benchmarkgroup(a.benchmarkgroup, b.benchmarkgroup),
        a.date,
        a.julia_commit,
        a.vinfo,
        a.benchmarkconfig
    )
end

function merge_results(input::Vector{BenchmarkResults})
    # That varargs is the only parameter may be a bit strange in Julia
    isempty(input) && error("When trying to merge BenchmarkResults, receive 0 results")
    return reduce(merge_results, input)
end

function merge_results(file_a::String, file_b::String)::BenchmarkResults
    isfile(file_a) || error("the benchmark result json ($(file_a)) doesn't exist")
    isfile(file_b) || error("the benchmark result json ($(file_b)) doesn't exist")
    a = readresults(file_a)
    b = readresults(file_b)
    return merge_results(a, b)
end

function merge_results(a::BenchmarkResults, file_b::String)::BenchmarkResults
    isfile(file_b) || error("the benchmark result json ($(file_b)) doesn't exist")
    b = readresults(file_b)
    return merge_results(a, b)
end

function merge_results(input_result_files::Vector{String})::BenchmarkResults
    isempty(input_result_files) && error("When trying to merge BenchmarkResults, receive 0 results")
    return reduce(merge_results, input_result_files)
end


"""
    markdown_report(judgement::BenchmarkJudgement)

Return markdown content (String) by the result of BenchmarkTools.judge.
"""
function markdown_report(judgement::BenchmarkJudgement)
    md = sprint(printresultmd, CIResult(judgement = judgement))
    md = replace(md, ":x:" => "❌")
    md = replace(md, ":white_check_mark:" => "✅")
    return md
end


"""
    display_markdown_report(report_md)

Display the content of the report after parsed by Markdown.parse.

* report_md: the result of [`markdown_report`](@ref)
"""
display_markdown_report(report_md) = display(Markdown.parse(report_md))
